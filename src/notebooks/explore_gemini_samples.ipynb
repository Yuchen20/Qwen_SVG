{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2c3e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "1.  Visual Elements: Cerulean sky and rolling green hills.\n",
      "2.  Color Palette: Cerulean (light blue), various shades of green for the hills.\n",
      "3.  Layout: Sky occupies the top portion, hills the bottom. Overlapping ellipses create the rolling effect.\n",
      "4.  Base Shapes: `rect` for the sky, multiple `ellipse` shapes for the hills.\n",
      "5.  Details: Overlap the hills and use varied green tones.\n",
      "6.  Transformations: None needed.\n",
      "7.  Review: Simple, clean, and accurately represents the scene.\n",
      "</reasoning>\n",
      "\n",
      "\n",
      "<svg viewBox=\"0 0 256 256\" width=\"256\" height=\"256\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "\n",
      "  <!-- Sky background -->\n",
      "  <rect x=\"0\" y=\"0\" width=\"256\" height=\"150\" fill=\"cerulean\" />\n",
      "\n",
      "  <!-- First rolling hill (dark green) -->\n",
      "  <ellipse cx=\"128\" cy=\"180\" rx=\"140\" ry=\"50\" fill=\"forestgreen\" />\n",
      "\n",
      "  <!-- Second rolling hill (medium green) -->\n",
      "  <ellipse cx=\"128\" cy=\"200\" rx=\"140\" ry=\"50\" fill=\"seagreen\" />\n",
      "\n",
      "  <!-- Third rolling hill (light green) -->\n",
      "  <ellipse cx=\"128\" cy=\"220\" rx=\"140\" ry=\"50\" fill=\"mediumseagreen\" />\n",
      "\n",
      "</svg>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_FILE = os.path.join(os.path.dirname(__name__), '..', '..', 'data', \"svg_results.jsonl\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(DATA_FILE, lines=True)\n",
    "\n",
    "# # remove the ```xml and ``` in the svg_content column\n",
    "# df['svg_content'] = df['svg_content'].str.replace('```xml', '', regex=False).str.replace('```', '', regex=False)\n",
    "# df['svg_content'] = df['svg_content'].str.strip()\n",
    "\n",
    "\n",
    "# # save it back to a new jsonl file\n",
    "# df.to_json(DATA_FILE, orient='records', lines=True)\n",
    "# print(f\"Saved cleaned data to {DATA_FILE}\")\n",
    "\n",
    "print(df.svg_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ab1b4",
   "metadata": {},
   "source": [
    "# Investigate Training Error\n",
    "\n",
    "We're encountering the following error when training:\n",
    "\n",
    "```\n",
    "ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [text]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\n",
    "```\n",
    "\n",
    "This is happening because our dataset has a column named 'text', but the model's forward method expects inputs like 'input_ids', 'attention_mask', etc. Let's explore the issue and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the dataset format issue\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Create a small sample dataset similar to what we have in SVGDataset\n",
    "sample_data = [\n",
    "    {\"text\": \"This is a sample instruction and response.\"}, \n",
    "    {\"text\": \"This is another sample.\"}\n",
    "]\n",
    "dataset = Dataset.from_list(sample_data)\n",
    "print(\"Original dataset format:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca832c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. The issue is that HF Trainer expects tokenized inputs, not raw text\n",
    "# Let's show how to fix this by properly tokenizing\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.5B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "# Function to tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=False, truncation=True, max_length=1024)\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"\\nTokenized dataset format (this is what Trainer expects):\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127048a0",
   "metadata": {},
   "source": [
    "## Solution: Fix the SVGDataset Class\n",
    "\n",
    "The issue is in our `SVGDataset.prepare_dataset()` method. We need to add a tokenization step after creating the dataset with the raw text. Here's how to fix the `train_svg_model.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a979244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the prepare_dataset method in SVGDataset class with:\n",
    "\n",
    "def prepare_dataset(self) -> DatasetDict:\n",
    "    \"\"\"Prepare the dataset for training by converting to HF Dataset format and splitting.\"\"\"\n",
    "    formatted_data = []\n",
    "    for item in self.data:\n",
    "        prompt = PROMPT_TEMPLATE.format(description=item['description'])\n",
    "        response = f\"{item['svg_content']}\"\n",
    "        \n",
    "        # Format as instruction format\n",
    "        formatted_data.append({\n",
    "            \"text\": f\"{prompt}\\n\\n{response}\",\n",
    "        })\n",
    "    \n",
    "    # Convert to HF Dataset\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        # This is for causal language modeling, so we don't need labels\n",
    "        return self.tokenizer(examples[\"text\"], padding=False, truncation=True, max_length=self.max_length)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Split into train/validation/test (90/5/5)\n",
    "    splits = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "    test_valid = splits['test'].train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        'train': splits['train'],\n",
    "        'validation': test_valid['train'], \n",
    "        'test': test_valid['test']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a566cd3",
   "metadata": {},
   "source": [
    "## Alternative Fix with TrainingArguments\n",
    "\n",
    "Alternatively, we can set `remove_unused_columns=False` in the TrainingArguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef813d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the train function, modify the TrainingArguments as follows:\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"valid_svg_ratio\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    fp16=fp16,\n",
    "    report_to=\"none\",  # We'll use custom W&B logging\n",
    "    remove_unused_columns=False,  # Add this line\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0a310",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "The first approach (tokenizing the dataset) is generally better because:\n",
    "\n",
    "1. It's more efficient - tokenization happens once during dataset preparation rather than every batch\n",
    "2. It's more explicit about what data the model is actually using\n",
    "3. It's the standard way to prepare datasets for HuggingFace Trainer\n",
    "\n",
    "Let's implement this fix in the `train_svg_model.py` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (Qwen_SVG)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
